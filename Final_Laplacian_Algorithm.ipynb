{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAR6m56tHOjY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from functools import partial\n",
        "\n",
        "# --- Closeness Metric Functions ---\n",
        "\n",
        "def rbf_kernel(squared_distances, t=1.0):\n",
        "    \"\"\"\n",
        "    Implements the default RBF (Gaussian) kernel for sample similarity S_ij.\n",
        "    S_ij = exp(-||x_i - x_j||^2 / t)\n",
        "    \"\"\"\n",
        "    return np.exp(-squared_distances / t)\n",
        "\n",
        "# --- Laplacian Score Calculation Function ---\n",
        "\n",
        "def calculate_laplacian_score(X, k, closeness_metric_func, **metric_params):\n",
        "    \"\"\"\n",
        "    Computes the Laplacian Score (Lr) for each feature using a user-defined\n",
        "    closeness metric function to build the similarity matrix S.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    # --- 1. Construct the nearest neighbor graph G and weight matrix S ---\n",
        "\n",
        "    # 1.1 Calculate all pairwise squared Euclidean distances (R_sq)\n",
        "    sum_sq_X = np.sum(X**2, axis=1, keepdims=True)\n",
        "    R_sq = sum_sq_X + sum_sq_X.T - 2 * np.dot(X, X.T)\n",
        "    R_sq[R_sq < 0] = 0\n",
        "\n",
        "    # 1.2 Find k-NN indices (required for graph structure)\n",
        "    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
        "    distances, indices = nbrs.kneighbors(X)\n",
        "\n",
        "    # 1.3 Initialize and populate S symmetrically using the custom metric\n",
        "    S = np.zeros((m, m))\n",
        "    for i in range(m):\n",
        "        neighbor_indices = indices[i, 1:]\n",
        "\n",
        "        weights = closeness_metric_func(R_sq[i, neighbor_indices], **metric_params)\n",
        "\n",
        "        S[i, neighbor_indices] = weights\n",
        "        S[neighbor_indices, i] = weights\n",
        "\n",
        "    # --- 2. D and L (Vectorized) ---\n",
        "    D_vector = np.sum(S, axis=1)\n",
        "    D = np.diag(D_vector)\n",
        "    L = D - S\n",
        "\n",
        "    d_sum = np.sum(D_vector)\n",
        "    if d_sum == 0:\n",
        "         return np.zeros(n)\n",
        "\n",
        "    # --- 3. Weighted Mean Removal (tilde(F)) (Vectorized) ---\n",
        "    M = (X.T @ D_vector) / d_sum\n",
        "    tilde_F = X - np.outer(np.ones(m), M)\n",
        "\n",
        "    # --- 4. Laplacian Score Lr (Fully Vectorized Matrix Products) ---\n",
        "\n",
        "    # Numerator Matrix: tilde(F)^T L tilde(F). Diagonal contains all numerators.\n",
        "    Numerator_Matrix = tilde_F.T @ L @ tilde_F\n",
        "    Numerator_vector = np.diag(Numerator_Matrix)\n",
        "\n",
        "    # Denominator Matrix: tilde(F)^T D tilde(F). Diagonal contains all denominators (Weighted Variance).\n",
        "    Denominator_Matrix = tilde_F.T @ D @ tilde_F\n",
        "    Denominator_vector = np.diag(Denominator_Matrix)\n",
        "\n",
        "    # Final Lr: Element-wise division with safety check\n",
        "    L_r_scores = np.divide(Numerator_vector, Denominator_vector,\n",
        "                           out=np.zeros_like(Numerator_vector, dtype=float),\n",
        "                           where=Denominator_vector != 0)\n",
        "\n",
        "    return L_r_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder data: 10 samples, 4 features\n",
        "X_example = np.array([\n",
        "    [5.1, 3.5, 1.4, 0.2],\n",
        "    [4.9, 3.0, 1.4, 0.2],\n",
        "    [4.7, 3.2, 1.3, 0.2],\n",
        "    [4.6, 3.1, 1.5, 0.2],\n",
        "    [5.0, 3.6, 1.4, 0.2],\n",
        "    [7.0, 3.2, 4.7, 1.4],\n",
        "    [6.4, 3.2, 4.5, 1.5],\n",
        "    [6.9, 3.1, 4.9, 1.5],\n",
        "    [5.5, 2.3, 4.0, 1.3],\n",
        "    [6.5, 2.8, 4.6, 1.5]\n",
        "])\n",
        "\n",
        "k_neighbors = 5\n",
        "\n",
        "# Call with RBF kernel and parameter t=1.0\n",
        "scores_rbf = calculate_laplacian_score(\n",
        "    X_example,\n",
        "    k_neighbors,\n",
        "    closeness_metric_func=rbf_kernel,\n",
        "    t=1.0\n",
        ")\n",
        "\n",
        "print(f\"Scores (RBF Kernel): {scores_rbf}\")\n",
        "# Scores (RBF Kernel): [0.08395097 0.89473051 0.00748533 0.00391253]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo3M_KFqHVrL",
        "outputId": "dbdeb22d-75d5-4b42-cfc3-4d458c8a6592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores (RBF Kernel): [0.08395097 0.89473051 0.00748533 0.00391253]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fisher_supervised_weight_matrix(labels):\n",
        "    \"\"\"\n",
        "    Computes the supervised weight matrix S based on class labels, as defined\n",
        "    in the Fisher Score section (Eq. 5).\n",
        "\n",
        "    This function generates the full S matrix, bypassing k-NN and R_sq calculation.\n",
        "\n",
        "    Args:\n",
        "        labels (numpy.ndarray): A 1D array of class labels for the data samples (y_i).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The weight matrix S, shape (m, m).\n",
        "    \"\"\"\n",
        "    m = len(labels)\n",
        "    S = np.zeros((m, m))\n",
        "\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    label_to_n = dict(zip(unique_labels, counts))\n",
        "\n",
        "    # Fully vectorized calculation of S\n",
        "    for label in unique_labels:\n",
        "        # Find indices of all samples belonging to this class 'l'\n",
        "        indices = np.where(labels == label)[0]\n",
        "        n_l = label_to_n[label]\n",
        "        weight = 1.0 / n_l\n",
        "\n",
        "        # S_ij = 1/n_l for all pairs (i, j) within the same class 'l'\n",
        "        # This creates an n_l x n_l block matrix (S_l) where every entry is 1/n_l\n",
        "        S[np.ix_(indices, indices)] = weight\n",
        "\n",
        "    return S"
      ],
      "metadata": {
        "id": "7oGhoL-oHqX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_laplacian_score_supervised(X, labels):\n",
        "    \"\"\"\n",
        "    Computes the Laplacian Score (Lr) using the supervised S matrix\n",
        "    defined in the paper's Fisher Score section (Eq. 5).\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): The data matrix, shape (m_samples, n_features).\n",
        "        labels (numpy.ndarray): A 1D array of class labels (y_i).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A vector of Laplacian Scores, Lr, for each feature.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    # --- 1. Compute S directly from labels ---\n",
        "    S = fisher_supervised_weight_matrix(labels)\n",
        "\n",
        "    # --- 2. D and L (Vectorized) ---\n",
        "    D_vector = np.sum(S, axis=1) # Note: For this specific S, D_vector will be all ones if labels are valid\n",
        "    D = np.diag(D_vector)\n",
        "    L = D - S\n",
        "\n",
        "    d_sum = np.sum(D_vector)\n",
        "    if d_sum == 0:\n",
        "         return np.zeros(n)\n",
        "\n",
        "    # --- 3. Weighted Mean Removal (tilde(F)) (Vectorized) ---\n",
        "    M = (X.T @ D_vector) / d_sum\n",
        "    tilde_F = X - np.outer(np.ones(m), M)\n",
        "\n",
        "    # --- 4. Laplacian Score Lr (Fully Vectorized Matrix Products) ---\n",
        "    Numerator_Matrix = tilde_F.T @ L @ tilde_F\n",
        "    Numerator_vector = np.diag(Numerator_Matrix)\n",
        "\n",
        "    Denominator_Matrix = tilde_F.T @ D @ tilde_F\n",
        "    Denominator_vector = np.diag(Denominator_Matrix)\n",
        "\n",
        "    L_r_scores = np.divide(Numerator_vector, Denominator_vector,\n",
        "                           out=np.zeros_like(Numerator_vector, dtype=float),\n",
        "                           where=Denominator_vector != 0)\n",
        "\n",
        "    return L_r_scores"
      ],
      "metadata": {
        "id": "4SOm7MKEIVnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison Example"
      ],
      "metadata": {
        "id": "KMvQSOxiImax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data matrix (6 samples, 2 features)\n",
        "X_comp = np.array([\n",
        "    [1.0, 9.0], [1.5, 8.5], [1.2, 9.5],\n",
        "    [8.0, 1.0], [8.5, 1.5], [8.2, 0.8]\n",
        "])\n",
        "\n",
        "# Class labels (n_0 = 3, n_1 = 3)\n",
        "labels_comp = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "1Hd-wAJjInsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}